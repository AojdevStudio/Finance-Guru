# Testing Strategy & Framework
> Auto-generated by codebase mapper

## Overview
Finance Guru™ uses pytest with fixtures, mocking, and organized test structure. Tests validate Pydantic models, calculator logic, and CLI integration across 18 test files organized by concern. Coverage includes unit tests, edge cases, integration points, and validation.

## Test Framework & Dependencies

- **Framework**: pytest (9.0.2+)
- **Coverage**: pytest-cov (7.0.0+)
- **Mocking**: unittest.mock (standard library)
- **Configuration**: pyproject.toml with pytest.ini_options

**Configuration File**: `/Users/ossieirondi/Documents/Irondi-Household/family-office/pyproject.toml`

```toml
[tool.pytest.ini_options]
testpaths = ["tests/python"]
python_files = ["test_*.py"]
python_functions = ["test_*"]
addopts = "-v --tb=short"
pythonpath = ["."]
markers = [
    "integration: marks tests as integration tests (require real API keys)",
]
```

## Test Directory Structure

```
tests/
└── python/
    ├── test_risk_metrics.py              # Unit tests for risk calculations
    ├── test_input_validation.py          # Unit tests for data validation
    ├── test_options_chain.py             # Unit tests for options models
    ├── test_itc_risk.py                  # Unit + integration (ITC API mocking)
    ├── test_compliance_officer_itc_integration.py  # Integration workflow
    ├── test_mcp_json_generation.py       # YAML/JSON generation + fixtures
    ├── test_yaml_generation.py           # YAML generation tests
    ├── test_env_setup.py                 # Environment setup validation
    ├── test_no_hardcoded_references.py   # No hardcoded paths/refs
    ├── test_onboarding_cli_structure.py  # CLI structure validation
    ├── test_onboarding_summary.py        # Onboarding workflow
    ├── test_debt_profile_section.py      # Debt profile tests + fixtures
    ├── test_cash_flow_section.py         # Cash flow section tests
    ├── test_liquid_assets_section.py     # Liquid assets section tests
    ├── test_investment_portfolio_section.py
    ├── test_preferences_section.py
    ├── test_progress_persistence.py      # State persistence tests
    └── test_claude_md_template.py        # Template validation
```

**Note**: No `conftest.py` currently exists; fixtures are defined within test files.

## Running Tests

### Run All Tests

```bash
# Run all tests with verbose output
uv run pytest

# Run with short traceback format
uv run pytest --tb=short

# Run with more verbose output
uv run pytest -vv
```

### Run Specific Test Files

```bash
# Single test file
uv run pytest tests/python/test_risk_metrics.py

# Multiple files
uv run pytest tests/python/test_risk_metrics.py tests/python/test_input_validation.py
```

### Run Specific Test Classes/Functions

```bash
# Specific test class
uv run pytest tests/python/test_risk_metrics.py::TestRiskMetricsBasics

# Specific test function
uv run pytest tests/python/test_risk_metrics.py::TestRiskMetricsBasics::test_sharpe_ratio_calculation
```

### Run Tests by Marker

```bash
# Skip integration tests (run only unit tests)
uv run pytest -m "not integration"

# Run only integration tests
uv run pytest -m integration
```

### Coverage Analysis

```bash
# Generate coverage report in terminal
uv run pytest --cov=src --cov-report=term

# Generate HTML coverage report
uv run pytest --cov=src --cov-report=html

# Open coverage report
open htmlcov/index.html
```

## Test Organization Patterns

### 1. Test Classes by Concern

Tests are organized into logical test classes:

```python
# tests/python/test_risk_metrics.py

class TestRiskMetricsBasics:
    """Basic validation tests for risk metric calculations."""

    def test_sharpe_ratio_calculation(self):
        """Sharpe ratio should be (return - risk_free) / volatility."""
        annual_return = 0.10
        risk_free_rate = 0.02
        volatility = 0.15

        sharpe = (annual_return - risk_free_rate) / volatility

        assert abs(sharpe - 0.533) < 0.01

    def test_max_drawdown_from_peak(self):
        """Max drawdown measures largest peak-to-trough decline."""
        prices = np.array([100, 110, 120, 100, 96, 105, 115])

        peak = np.maximum.accumulate(prices)
        drawdown = (peak - prices) / peak
        max_dd = np.max(drawdown)

        assert abs(max_dd - 0.20) < 0.01


class TestRiskMetricsEdgeCases:
    """Edge case handling for risk metrics."""

    def test_zero_volatility_sharpe(self):
        """Zero volatility should handle gracefully."""
        returns = np.array([0.01, 0.01, 0.01, 0.01])
        volatility = np.std(returns)

        assert volatility == 0.0
```

### 2. Test Naming Convention

```
test_<what_is_being_tested>_<expected_outcome>
test_<noun>_<verb>_<constraint>

Examples:
- test_sharpe_ratio_calculation
- test_max_drawdown_from_peak
- test_zero_volatility_sharpe
- test_price_series_requires_positive_prices
- test_strike_must_be_positive
```

### 3. Triple-A Pattern (Arrange-Act-Assert)

All tests follow this pattern:

```python
def test_var_95_basic(self):
    """VaR 95% should capture 5th percentile of returns."""

    # ARRANGE: Set up test data
    np.random.seed(42)
    returns = np.random.normal(0.001, 0.02, 1000)

    # ACT: Perform the calculation
    var_95 = np.percentile(returns, 5)

    # ASSERT: Verify the result
    assert var_95 < 0
    assert var_95 > -0.05
```

## Fixture Patterns

Fixtures provide reusable test data and setup/teardown.

### Pytest Fixture Definition

```python
import pytest

@pytest.fixture
def valid_user_data_with_all_mcp() -> UserDataInput:
    """Create valid user data with all MCP servers."""
    return UserDataInput(
        identity=UserIdentityInput(user_name="Alex", language="English"),
        liquid_assets=LiquidAssetsInput(
            total=50000.0,
            accounts_count=5,
            average_yield=0.045,
        ),
        portfolio=InvestmentPortfolioInput(
            total_value=500000.0,
            brokerage="Fidelity",
            has_retirement=True,
            # ... more fields
        ),
        # ... more sections
    )
```

### Using Fixtures in Tests

```python
class TestMCPGeneration:
    """Tests using fixtures."""

    def test_mcp_generation_with_all_servers(
        self,
        valid_user_data_with_all_mcp: UserDataInput
    ):
        """Test MCP config generation with all optional servers."""
        generator = YAMLGenerator()
        result = generator.generate_mcp_config(valid_user_data_with_all_mcp)

        assert result is not None
        assert "alphavantage" in result
        assert "brightdata" in result

    def test_mcp_generation_without_optional(
        self,
        valid_user_data_no_optional_mcp: UserDataInput
    ):
        """Test MCP config without optional servers."""
        generator = YAMLGenerator()
        result = generator.generate_mcp_config(valid_user_data_no_optional_mcp)

        assert result is not None
        assert "alphavantage" not in result
```

### Fixture Location Convention

- **File-scoped fixtures**: Define in the same test file
- **Shared fixtures**: Create a `conftest.py` in test directory (currently not used)
- **External fixtures**: Import from utilities

**Example Fixture File**: `/Users/ossieirondi/Documents/Irondi-Household/family-office/tests/python/test_mcp_json_generation.py`

Fixtures defined at lines 40-111 provide comprehensive test data for MCP generation tests.

## Pydantic Model Validation Tests

Tests validate that Pydantic models reject invalid input and accept valid input.

### Model Validation Pattern

```python
# tests/python/test_input_validation.py

from pydantic import ValidationError

class TestPydanticModelValidation:
    """Test Pydantic models catch invalid input."""

    def test_price_series_requires_positive_prices(self):
        """Prices must be positive."""
        with pytest.raises(ValidationError, match="positive"):
            PriceSeriesInput(
                ticker="TSLA",
                prices=[100.0] * 9 + [-50.0],  # Negative price
                dates=[date(2025, 10, i) for i in range(10, 20)],
            )

    def test_price_series_requires_sorted_dates(self):
        """Dates must be chronological."""
        with pytest.raises(ValidationError, match="chronological"):
            dates_list = [date(2025, 10, i) for i in range(10, 19)]
            dates_list.append(date(2025, 10, 12))  # Out of order
            PriceSeriesInput(
                ticker="AAPL",
                prices=[100.0] * 10,
                dates=dates_list,
            )

    def test_price_series_requires_aligned_data(self):
        """Prices and dates must have same length."""
        with pytest.raises(ValidationError, match="Length mismatch"):
            PriceSeriesInput(
                ticker="NVDA",
                prices=[100.0] * 10,  # 10 prices
                dates=[date(2025, 10, i) for i in range(10, 21)],  # 11 dates
            )
```

### ValidationError Testing

```python
# Check for specific validation error
with pytest.raises(ValidationError) as exc_info:
    invalid_model = MyModel(invalid_field=value)

# Access the error details
errors = exc_info.value.errors()
assert len(errors) == 1
assert errors[0]['field'] == 'expected_field_name'
```

### Field Constraints Testing

```python
def test_strike_must_be_positive(self):
    """Strike price must be greater than zero."""
    from pydantic import ValidationError

    with pytest.raises(ValidationError):
        OptionContractData(
            contract_symbol="TEST",
            expiration="2026-04-17",
            strike=0.0,  # Invalid: must be > 0
            otm_pct=10.0,
            days_to_expiry=30,
            last_price=1.0,
            bid=0.9,
            ask=1.1,
            mid=1.0,
            total_cost=100.0,
        )
```

**Example Test File**: `/Users/ossieirondi/Documents/Irondi-Household/family-office/tests/python/test_input_validation.py`

**Example Model Tests**: `/Users/ossieirondi/Documents/Irondi-Household/family-office/tests/python/test_options_chain.py` (lines 27-80)

## Mocking Patterns

Use `unittest.mock` for external dependencies (API calls, file I/O).

### Mocking External API Calls

```python
# tests/python/test_itc_risk.py

from unittest.mock import MagicMock, patch
import pytest

class TestITCRiskAPI:
    """Tests for ITC Risk API integration with mocking."""

    @patch("src.analysis.itc_risk.requests.get")
    def test_successful_api_response(self, mock_get):
        """Test successful API response handling."""
        # ARRANGE: Mock the requests.get call
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "symbol": "TSLA",
            "risk_score": 65.5,
            "risk_bands": [
                {"lower": 0, "upper": 30, "label": "Low Risk"},
                {"lower": 30, "upper": 70, "label": "Medium Risk"},
            ],
        }
        mock_get.return_value = mock_response

        # ACT: Call the function
        calculator = ITCRiskCalculator(api_key="test-key")
        result = calculator.get_risk(symbol="TSLA", universe="tradfi")

        # ASSERT: Verify behavior
        assert result.current_risk_score == 65.5
        assert len(result.risk_bands) == 2
        mock_get.assert_called_once()

    @patch("src.analysis.itc_risk.requests.get")
    def test_api_error_handling(self, mock_get):
        """Test handling of API errors."""
        mock_get.side_effect = requests.RequestException("Connection error")

        calculator = ITCRiskCalculator(api_key="test-key")

        with pytest.raises(ValueError):
            calculator.get_risk(symbol="TSLA", universe="tradfi")

    @patch("src.analysis.itc_risk.requests.get")
    @patch("src.analysis.itc_risk.time.sleep")
    def test_retry_logic_with_backoff(self, mock_sleep, mock_get):
        """Test retry logic with exponential backoff."""
        # First two calls fail, third succeeds
        mock_get.side_effect = [
            requests.RequestException("Timeout"),
            requests.RequestException("Timeout"),
            MagicMock(status_code=200, json=lambda: {"symbol": "TSLA", "risk_score": 65}),
        ]

        calculator = ITCRiskCalculator(api_key="test-key", max_retries=3)
        result = calculator.get_risk(symbol="TSLA", universe="tradfi")

        assert result.current_risk_score == 65
        assert mock_get.call_count == 3
        assert mock_sleep.call_count == 2  # Sleep called twice (backoff)
```

### Mocking File I/O

```python
@patch("builtins.open", create=True)
def test_save_to_file(self, mock_open):
    """Test saving output to file."""
    mock_file = MagicMock()
    mock_open.return_value.__enter__.return_value = mock_file

    # Call function that saves to file
    result = calculator.calculate_and_save(data, output_path="test.json")

    # Verify file was opened and written
    mock_open.assert_called_with("test.json", "w")
    mock_file.write.assert_called()
```

### Mocking Partial Classes

```python
@patch("src.analysis.itc_risk.requests.get")
def test_api_integration(self, mock_get):
    """Mock only the requests.get, real calculator code runs."""
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {"symbol": "TSLA", "risk_score": 65}
    mock_get.return_value = mock_response

    # Real ITCRiskCalculator code runs, only API call is mocked
    calculator = ITCRiskCalculator(api_key="test-key")
    result = calculator.get_risk(symbol="TSLA", universe="tradfi")

    assert result.symbol == "TSLA"
```

**Example Mocking File**: `/Users/ossieirondi/Documents/Irondi-Household/family-office/tests/python/test_itc_risk.py`

**Example Integration Mocking**: `/Users/ossieirondi/Documents/Irondi-Household/family-office/tests/python/test_compliance_officer_itc_integration.py`

## Test Categories

### 1. Unit Tests

Test individual functions/methods in isolation.

```python
# tests/python/test_risk_metrics.py

class TestRiskMetricsBasics:
    """Unit tests for risk metric calculations."""

    def test_sharpe_ratio_calculation(self):
        """Test Sharpe ratio formula in isolation."""
        annual_return = 0.10
        risk_free_rate = 0.02
        volatility = 0.15

        sharpe = (annual_return - risk_free_rate) / volatility

        assert abs(sharpe - 0.533) < 0.01
```

### 2. Model Validation Tests

Test Pydantic model constraints and validators.

```python
# tests/python/test_options_chain.py

class TestOptionContractDataModel:
    """Validate OptionContractData Pydantic model constraints."""

    def test_valid_contract_creation(self):
        """Valid data should instantiate successfully."""
        contract = OptionContractData(
            contract_symbol="QQQ260417P00400000",
            strike=400.0,
            otm_pct=15.5,
            # ... other fields
        )

        assert contract.strike == 400.0
        assert contract.otm_pct == 15.5
```

### 3. Integration Tests

Test multiple components working together, often mocking external dependencies.

```python
# tests/python/test_compliance_officer_itc_integration.py

@pytest.mark.integration
class TestComplianceOfficerITCIntegration:
    """Integration tests for Compliance Officer using ITC Risk."""

    @patch("src.analysis.itc_risk.requests.get")
    def test_full_compliance_workflow(self, mock_get):
        """Test complete compliance check workflow."""
        # Mock API response
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "symbol": "TSLA",
            "risk_score": 75.0,  # Medium-high risk
        }
        mock_get.return_value = mock_response

        # Run workflow
        compliance_checker = ComplianceOfficer()
        result = compliance_checker.assess_position(
            ticker="TSLA",
            position_size=10000.0
        )

        # Verify decision logic
        assert result.is_compliant == False
        assert result.reason == "Position risk exceeds compliance threshold"
```

### 4. Marked Integration Tests

Use pytest markers to distinguish integration tests (require API keys):

```python
@pytest.mark.integration
def test_fetch_real_market_data():
    """Integration test requiring real market data."""
    prices = fetch_prices("SPY", days=30)
    assert len(prices) > 0
```

Run only unit tests:
```bash
uv run pytest -m "not integration"
```

**Example Integration Test File**: `/Users/ossieirondi/Documents/Irondi-Household/family-office/tests/python/test_compliance_officer_itc_integration.py`

## Edge Case Testing

Always test edge cases and error conditions.

### Edge Case Patterns

```python
class TestRiskMetricsEdgeCases:
    """Edge case handling for risk metrics."""

    def test_zero_volatility_sharpe(self):
        """Zero volatility (constant returns) should handle gracefully."""
        returns = np.array([0.01, 0.01, 0.01, 0.01])
        volatility = np.std(returns)

        assert volatility == 0.0
        # Real implementation should return inf or handle specially

    def test_empty_returns_array(self):
        """Empty returns should raise or return NaN."""
        returns = np.array([])

        with pytest.raises((ValueError, IndexError)):
            np.percentile(returns, 5)

    def test_minimum_data_requirement(self):
        """Data below minimum threshold should fail validation."""
        with pytest.raises(ValidationError):
            PriceDataInput(
                ticker="TSLA",
                prices=[100.0] * 29,  # Only 29 points (min is 30)
                dates=[date(2025, 10, i) for i in range(10, 39)],
            )

    def test_extreme_values(self):
        """Test with extreme but valid values."""
        prices = [0.01] * 10 + [1000000.0] * 10  # Very low to very high

        data = PriceDataInput(
            ticker="EXTREME",
            prices=prices,
            dates=[date(2025, 10, i) for i in range(20)],
        )

        assert data.prices[0] == 0.01
        assert data.prices[-1] == 1000000.0
```

## Test Coverage

### Coverage Configuration

pytest-cov is included in dev dependencies:

```bash
# Generate coverage for src/
uv run pytest --cov=src --cov-report=term

# Generate HTML report
uv run pytest --cov=src --cov-report=html

# View coverage report
open htmlcov/index.html
```

### Target Coverage

```
src/models/     - 100%  (all Pydantic models must be tested)
src/analysis/   - 90%+  (calculators and CLIs)
src/strategies/ - 90%+  (strategy implementations)
src/utils/      - 85%+  (utility functions)
```

## Common Test Utilities

### Assertion Patterns

```python
# Numeric comparisons with tolerance
assert abs(calculated - expected) < 0.01

# Percentage comparisons
assert abs(sharpe_ratio - 0.533) / 0.533 < 0.05  # 5% tolerance

# String matching in exceptions
with pytest.raises(ValueError, match="positive"):
    invalid_data = PriceDataInput(...)

# List/dict assertions
assert len(results) == 3
assert "field_name" in result_dict
assert all(price > 0 for price in prices)

# Datetime/date comparisons
from datetime import date, timedelta
test_date = date(2025, 10, 15)
assert test_date > date(2025, 10, 14)
```

### Test Data Factories

Use fixtures to create consistent test data:

```python
@pytest.fixture
def valid_user_data_with_all_mcp() -> UserDataInput:
    """Standard user profile with all MCP servers."""
    return UserDataInput(
        identity=UserIdentityInput(user_name="TestUser"),
        # ... complete valid data ...
    )

@pytest.fixture
def valid_user_data_minimal() -> UserDataInput:
    """Minimal valid user profile."""
    return UserDataInput(
        identity=UserIdentityInput(user_name="Minimal"),
        # ... minimum required fields ...
    )
```

## Test File Checklist

Before committing tests, verify:

- [ ] Test class names start with `Test`
- [ ] Test functions start with `test_`
- [ ] Descriptive docstrings explain what is being tested
- [ ] Triple-A pattern (Arrange-Act-Assert) followed
- [ ] Edge cases included
- [ ] Assertions verify behavior, not implementation
- [ ] Fixtures reduce duplication
- [ ] Mocks used for external dependencies
- [ ] Integration tests marked with `@pytest.mark.integration`
- [ ] No hardcoded file paths or credentials
- [ ] Tests are isolated (order-independent)

## Running Full Test Suite

```bash
# Run all tests with coverage
uv run pytest --cov=src --cov-report=html

# Run only fast unit tests
uv run pytest -m "not integration"

# Run with detailed output
uv run pytest -vv

# Run specific test file
uv run pytest tests/python/test_risk_metrics.py -v

# Run with markers
uv run pytest -m integration -v
```

## Test File Summary

| Test File | Focus | Fixtures | Mocking |
|-----------|-------|----------|---------|
| `test_risk_metrics.py` | Risk calculation unit tests | None | None |
| `test_input_validation.py` | Pydantic model validation | None | None |
| `test_options_chain.py` | Options model validation | None | unittest.mock |
| `test_itc_risk.py` | ITC Risk API + calculations | None | requests.get |
| `test_compliance_officer_itc_integration.py` | Integration workflow | None | requests.get |
| `test_mcp_json_generation.py` | YAML generation + config | Multiple (lines 40-111) | None |
| `test_yaml_generation.py` | YAML output tests | None | None |
| `test_env_setup.py` | Environment validation | None | None |
| `test_no_hardcoded_references.py` | Code quality | None | None |
| `test_onboarding_cli_structure.py` | CLI structure | None | None |
| `test_onboarding_summary.py` | Onboarding workflow | None | None |
| `test_debt_profile_section.py` | Debt profile tests | Multiple | None |
| `test_cash_flow_section.py` | Cash flow section | None | None |
| `test_liquid_assets_section.py` | Liquid assets | None | None |
| `test_investment_portfolio_section.py` | Portfolio tests | None | None |
| `test_preferences_section.py` | Preferences tests | None | None |
| `test_progress_persistence.py` | State persistence | None | None |
| `test_claude_md_template.py` | Template validation | None | None |

**Total Test Count**: 18 test files

## Continuous Integration Ready

Tests are CI/CD ready with:
- Explicit test discovery via `pyproject.toml`
- Clear test organization
- Comprehensive fixtures
- Proper mocking for external dependencies
- Marked integration tests
- Coverage reporting support
