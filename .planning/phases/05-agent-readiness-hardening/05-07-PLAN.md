---
phase: 05-agent-readiness-hardening
plan: 07
type: execute
wave: 3
depends_on: ["05-03"]
files_modified:
  - tests/python/test_data_validator.py
  - tests/python/test_screener.py
  - tests/python/test_dashboard_inputs.py
  - tests/python/test_screener_inputs.py
autonomous: true

must_haves:
  truths:
    - "data_validator.py has test coverage above 80%"
    - "screener.py has test coverage above 80%"
    - "dashboard_inputs.py has test coverage above 70%"
    - "screener_inputs.py has test coverage above 80%"
  artifacts:
    - path: "tests/python/test_data_validator.py"
      provides: "Tests for data validator"
      contains: "class TestDataValidator"
    - path: "tests/python/test_screener.py"
      provides: "Tests for screener"
      contains: "class TestScreener"
    - path: "tests/python/test_dashboard_inputs.py"
      provides: "Tests for dashboard Pydantic models"
      contains: "class TestDashboardInputs"
    - path: "tests/python/test_screener_inputs.py"
      provides: "Tests for screener Pydantic models"
      contains: "class TestScreenerInputs"
  key_links:
    - from: "tests/python/test_data_validator.py"
      to: "src/utils/data_validator.py"
      via: "import and test DataValidator"
      pattern: "from src.utils.data_validator import"
    - from: "tests/python/test_screener.py"
      to: "src/utils/screener.py"
      via: "import and test Screener"
      pattern: "from src.utils.screener import"
---

<objective>
Write comprehensive test suites for the data validation, screener, and uncovered Pydantic model modules to bring them from 0% to 80%+ coverage.

Purpose: These modules handle data quality validation and stock screening. Testing them contributes ~494 statements toward the 80% coverage target, plus improves model validation confidence.
Output: Four new test files in tests/python/.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/codebase/TESTING.md
@src/utils/data_validator.py
@src/utils/screener.py
@src/models/dashboard_inputs.py
@src/models/screener_inputs.py
@src/models/factors_inputs.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write tests for data_validator.py and screener.py</name>
  <files>
    tests/python/test_data_validator.py
    tests/python/test_screener.py
  </files>
  <action>
  Read src/utils/data_validator.py (156 stmts) and src/utils/screener.py (267 stmts), then write test suites.

  **For test_data_validator.py:**
  1. Read data_validator.py to understand the DataValidator class
  2. Test validation methods:
     - Price data validation (positive prices, no NaN, chronological dates)
     - Return data validation (reasonable bounds, no infinite values)
     - Volume data validation (positive integers)
     - Data completeness checks (missing data detection)
     - Anomaly detection (outliers, gaps, suspicious values)
  3. Create mock DataFrames with known issues:
     - Valid clean data (passes all checks)
     - Data with NaN values
     - Data with negative prices
     - Data with dates out of order
     - Data with extreme outliers
  4. Test severity levels if present (warning vs error)
  5. Mock any external dependencies

  **For test_screener.py:**
  1. Read screener.py to understand the Screener class
  2. IMPORTANT: screener.py calls momentum and other calculators internally. Mock these calculator calls to avoid testing transitive dependencies.
  3. Test screening functionality:
     - Single-criteria screening (e.g., RSI < 30)
     - Multi-criteria screening
     - Result ranking/sorting
     - Recommendation generation (strong_buy, buy, hold, sell, strong_sell)
  4. Create mock market data for 5-10 tickers
  5. Mock all external API calls (yfinance, market_data.py)
  6. Test edge cases: empty ticker list, all tickers failing data fetch, single ticker

  **Mocking pattern for screener:**
  ```python
  @patch("src.utils.screener.yf.Ticker")
  @patch("src.utils.screener.MomentumCalculator")
  def test_screen_single_ticker(self, mock_momentum, mock_yf):
      mock_yf.return_value.history.return_value = self._mock_prices()
      mock_momentum.return_value.calculate_rsi.return_value = 25.0  # Oversold
      # ... test screening logic
  ```
  </action>
  <verify>
  - `uv run pytest tests/python/test_data_validator.py tests/python/test_screener.py -v` passes
  - `uv run pytest tests/python/test_data_validator.py --cov=src/utils/data_validator --cov-report=term-missing` shows >= 80%
  - `uv run pytest tests/python/test_screener.py --cov=src/utils/screener --cov-report=term-missing` shows >= 80%
  </verify>
  <done>test_data_validator.py covers data_validator.py at 80%+. test_screener.py covers screener.py at 80%+. All tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Write tests for uncovered Pydantic models</name>
  <files>
    tests/python/test_dashboard_inputs.py
    tests/python/test_screener_inputs.py
  </files>
  <action>
  Read the uncovered model files and write validation test suites.

  **For test_dashboard_inputs.py (99 stmts at 0%):**
  1. Read src/models/dashboard_inputs.py
  2. Test all Pydantic model classes defined in the file:
     - Valid instantiation with correct data
     - Reject invalid data (negative values, missing required fields)
     - Field validators (custom validators with @field_validator)
     - Computed fields / properties (test they return correct values)
     - Model serialization (model_dump_json, model_dump)
  3. Test each model class separately with its own test class

  **For test_screener_inputs.py (66 stmts at 0%):**
  1. Read src/models/screener_inputs.py
  2. Test:
     - ScreeningCriteria model (valid criteria, invalid bounds)
     - ScreeningResult model (valid results, Literal field validation)
     - Any custom validators
     - Edge cases for numeric constraints

  **Also cover factors_inputs.py (62 stmts at 0%) in the screener_inputs test file** since it is small:
  1. Read src/models/factors_inputs.py
  2. Add a TestFactorsInputs class to test_screener_inputs.py (or create test_factors_inputs.py if too large)
  3. Test FactorDataInput model validation

  **Model testing pattern:**
  ```python
  from pydantic import ValidationError

  class TestDashboardInputs:
      def test_valid_portfolio_snapshot(self):
          snapshot = PortfolioSnapshotInput(
              total_value=500000.0,
              cash_balance=50000.0,
              # ... valid fields
          )
          assert snapshot.total_value == 500000.0

      def test_reject_negative_portfolio_value(self):
          with pytest.raises(ValidationError):
              PortfolioSnapshotInput(total_value=-100.0, ...)
  ```
  </action>
  <verify>
  - `uv run pytest tests/python/test_dashboard_inputs.py tests/python/test_screener_inputs.py -v` passes
  - `uv run pytest tests/python/test_dashboard_inputs.py --cov=src/models/dashboard_inputs --cov-report=term-missing` shows >= 70%
  - `uv run pytest tests/python/test_screener_inputs.py --cov=src/models/screener_inputs --cov-report=term-missing` shows >= 80%
  </verify>
  <done>Dashboard and screener model tests written and passing. dashboard_inputs.py at 70%+, screener_inputs.py at 80%+, factors_inputs.py covered.</done>
</task>

</tasks>

<verification>
1. All four new test files pass
2. data_validator.py and screener.py at 80%+ coverage each
3. dashboard_inputs.py at 70%+, screener_inputs.py at 80%+
4. Full test suite still passes: `uv run pytest -x -q`
</verification>

<success_criteria>
Four new test files written and passing. Target coverage achieved for each module.
</success_criteria>

<output>
After completion, create `.planning/phases/05-agent-readiness-hardening/05-07-SUMMARY.md`
</output>
