---
phase: 05-agent-readiness-hardening
plan: 04
type: execute
wave: 3
depends_on: ["05-01"]
files_modified:
  - tests/python/test_risk_metrics.py
  - tests/python/test_momentum.py
  - tests/python/test_volatility.py
  - tests/python/test_correlation.py
  - tests/python/test_moving_averages.py
  - tests/python/test_market_data.py
autonomous: true

must_haves:
  truths:
    - "Core analysis modules (risk_metrics, momentum, volatility, correlation, moving_averages) have test files with meaningful coverage"
    - "All new tests pass alongside existing 440+ tests"
    - "Coverage for src/ has increased significantly from the 26% baseline"
  artifacts:
    - path: "tests/python/test_risk_metrics.py"
      provides: "Tests for risk metrics calculator"
      contains: "def test_"
    - path: "tests/python/test_momentum.py"
      provides: "Tests for momentum calculator"
      contains: "def test_"
    - path: "tests/python/test_volatility.py"
      provides: "Tests for volatility calculator"
      contains: "def test_"
    - path: "tests/python/test_correlation.py"
      provides: "Tests for correlation calculator"
      contains: "def test_"
    - path: "tests/python/test_moving_averages.py"
      provides: "Tests for moving averages calculator"
      contains: "def test_"
  key_links:
    - from: "tests/python/test_risk_metrics.py"
      to: "src/analysis/risk_metrics.py"
      via: "import and test calculator methods"
      pattern: "from src.analysis.risk_metrics import"
---

<objective>
Write comprehensive tests for the core analysis modules that currently have 0% coverage: risk_metrics, momentum, volatility, correlation, and moving_averages.

Purpose: Raise test coverage from 26% toward the 80% target. These are the largest untested modules in the codebase and represent the core analysis functionality.
Output: 5+ new test files covering the analysis and utility calculator classes.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-agent-readiness-hardening/05-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write tests for analysis calculators (risk_metrics, momentum, volatility, correlation)</name>
  <files>
    tests/python/test_risk_metrics.py
    tests/python/test_momentum.py
    tests/python/test_volatility.py
    tests/python/test_correlation.py
  </files>
  <action>
For each analysis module, create a test file that tests the calculator class methods with synthetic data (numpy/pandas generated). Do NOT use real API calls -- mock yfinance or use pre-built DataFrames.

**Test strategy per module:**
1. Read the source file to understand the calculator class API (methods, parameters, return types)
2. Create synthetic price/return data using `pd.DataFrame` with known values
3. Test each public method with:
   - Happy path (valid input, expected output range)
   - Edge cases (single data point, all-zero returns, NaN handling)
   - Known-answer tests where possible (e.g., Sharpe ratio of zero-return series = 0)

**For each test file:**
- Import the calculator class from src/
- Use pytest fixtures for common test data (synthetic price series, return series)
- Use `pytest.approx()` for floating point comparisons
- Mark any tests that need real API keys with `@pytest.mark.integration`
- Target 15-25 tests per module

**risk_metrics.py tests:** VaR, CVaR, Sharpe ratio, Sortino ratio, max drawdown, Calmar ratio, volatility, beta, alpha
**momentum.py tests:** RSI, MACD, Stochastic oscillator, Williams %R, ROC, confluence score
**volatility.py tests:** Bollinger Bands, ATR, historical volatility, Keltner channels, standard deviation, regime detection
**correlation.py tests:** Pearson correlation matrix, covariance matrix, diversification ratio, concentration

Do NOT test CLI wrappers (*_cli.py) -- those are excluded from coverage.
  </action>
  <verify>
`uv run pytest tests/python/test_risk_metrics.py tests/python/test_momentum.py tests/python/test_volatility.py tests/python/test_correlation.py -v` passes all new tests. Run `uv run pytest --cov=src --cov-report=term-missing` to measure coverage improvement.
  </verify>
  <done>Four analysis calculator test files exist with 15-25 tests each. All tests pass. Coverage for these modules is above 70%.</done>
</task>

<task type="auto">
  <name>Task 2: Write tests for utility calculators (moving_averages, market_data)</name>
  <files>
    tests/python/test_moving_averages.py
    tests/python/test_market_data.py
  </files>
  <action>
Same strategy as Task 1 for the utility modules:

**moving_averages.py tests:** SMA, EMA, WMA, HMA, golden/death cross detection. Use synthetic price series with known moving average values (e.g., SMA of [1,2,3,4,5] with period 3 = [NaN, NaN, 2, 3, 4]).

**market_data.py tests:** Mock yfinance calls. Test:
- get_prices returns DataFrame with expected columns
- Date range handling (--days parameter)
- Multiple ticker handling
- Error handling when ticker not found
- Cache behavior if applicable

Use `pytest-mock` (already in dev deps) to mock yfinance.download and yfinance.Ticker calls.

After writing all tests, run the FULL test suite:
```bash
uv run pytest --cov=src --cov-report=term-missing
```

Record the coverage percentage. Target: coverage should be significantly above 50% after this plan.
  </action>
  <verify>
`uv run pytest` passes ALL tests (old + new). `uv run pytest --cov=src --cov-report=term-missing` shows coverage above 50%. No regressions in existing tests.
  </verify>
  <done>Moving averages and market data test files exist. Full test suite passes. Coverage has increased substantially from 26% baseline.</done>
</task>

</tasks>

<verification>
1. `uv run pytest` passes all tests (old + new, 500+ total)
2. `uv run pytest --cov=src --cov-report=term-missing` shows coverage above 50%
3. New test files follow existing test patterns (test_ prefix, fixtures, approx for floats)
4. No real API calls in new tests (all mocked or synthetic data)
5. `uv run ruff check tests/` passes (new tests are lint-clean)
</verification>

<success_criteria>
Core analysis and utility modules have comprehensive test coverage. All tests pass. Coverage has increased significantly from the 26% baseline toward the 80% target.
</success_criteria>

<output>
After completion, create `.planning/phases/05-agent-readiness-hardening/05-04-SUMMARY.md`
</output>
